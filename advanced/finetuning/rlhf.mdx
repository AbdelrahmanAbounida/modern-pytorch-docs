---
title: "RLHF"
description: "Reinforcement Learning from Human Feedback"
---

# RLHF

Reward modeling, PPO training, alignment techniques.
