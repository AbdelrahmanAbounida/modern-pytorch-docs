---
title: "Transformers"
description: "Attention is all you need - the dominant architecture"
---

# Transformers

Self-attention, multi-head attention, positional encoding, encoder-decoder architecture.
